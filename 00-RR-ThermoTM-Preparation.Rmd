---
title: "00-RR-ThermoTM-Preparation"
author: "Anoff Nicholas Cobblah"
date: "January 21, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

*********************************************************************************************************************************************
*********************************************************************************************************************************************
*********************************************************************************************************************************************

# Introduction
PURPOSE: This document demonstrates an application of the computational methodology which has emerged from my current dissertation project *The Work of Scientific Play in Nineteenth-Century Britain*: a methodology which I call **Recreational Reckoning**.

## Preparation

### Experimental Question
R can be a powerful tool for better understanding texts. It isn't always necessary to have a fully testable hypothesis in mind; visualizing texts can be a powerful tool for discovery, especially when you are willing to have fun, exploring the many ways in which one can customize your analysis.  On the other hand, because the data can be easily manipulated, one can easily fall into the trap of thinking they observe a feature in the text, and then manipulating the text to draw out that feature.  Fishing for information that supports a theory one already holds is a real problem in computational criticism.

Unlike the social sciences, however, the humanities more generally proceed not through testable and reproducable experiments, but through the development of IDEAS.  **Recreational computational criticism therefore asks only that you choose one question that your analysis will answer.  Questions such as: "Does Dickens's Bleak House include more masculine or feminine pronouns?"; "What topics are central to the Sherlock Holmes canon?"; "Do novel titles become longer or shorter over the course of the nineteenth-century?" New features may become observable while pursuing this analysis. And it is up to the critic to theorize about what this newly visualized feature means.**

#### Why R?
Remember, R isn't the only tool one can use for visualizing texts. R computational methods shine when you have texts that are either too long to read quickly, or too many texts to read quickly. They are also useful when you have a specific methodology in mind or prioritize customizability in the data mining or the visualization.  For quick visualizations of things like word clouds, Voyant (https://voyant-tools.org) is probably a better. 

### Downloading R
The first step in using this methodology is obviously to download R.  This can be done here (https://www.r-project.org). Users will also find it helpful to download RStudio, an environment which will make running the code easier. (If you are reading this in R/RStudio, then congratulations on already having started!)

### Setting Directory
The first step in analyzing your data is choosing a workspace. **I recommend creating a new folder for each project.** This folder will be your *working directory.* The working directory in R is generallyset via the "setwd()" command. However, here, we're going to be working within an R Markdown File (.Rmd). R Markdowns rely on a package called knitr, which generally requires the R Markdown being stored in the location of your working directory. So I would recommend creating a new folder, and then downloading this R Markdown and saving it in the folder where you want to work. For example, you might create a folder called "data" on your computer desktop, in which case your working directory would be something like "C:/Users/Nick/Desktop/data". **You can check that your working directory is indeed in the right place by using the "getwd()" function below.** 

```{r directory, root.dir=TRUE}
getwd()
```

### Downloading Packages

The next step is to load in the packages that will be required. My methodology makes use of several packages, depending on what is required for the task.  Packages are initially loaded with the "install.packages()" function.  **HOWEVER, THIS STEP ONLY HAS TO BE COMPLETED ONCE.**

"ggplot2" is a package for data visualizations.  More information can be found here (https://cran.r-project.org/web/packages/ggplot2/index.html).

"pdftools" is a package for reading pdfs. In the past, you had to download a separate pdf reader, and it was a real pain. You, reader, are living in a golden age. Information on the package can be found here (https://cran.r-project.org/web/packages/pdftools/pdftools.pdf).

"plotly" is a package for creating interactive plots.

"quanteda" is a package for the quantitative analysis of texts.  More information can be found here (https://cran.r-project.org/web/packages/quanteda/quanteda.pdf).

"readr" is a package for reading in certain types of data. More information can be found here (https://cran.r-project.org/web/packages/readr/readr.pdf).

"SnowballC" is a package for stemming words (lemmatizing words, or basically cutting the ends off words as a way of lowering the dimensions of the data.  For instance, "working","worked", and "works" all become "work").

"tm" is a simple package for text mining. An introduction to the package can be found here (https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf).

"tokenizers" is a package which turns a text into a character vector.  An introduction to the package can be found here (https://cran.r-project.org/web/packages/tokenizers/vignettes/introduction-to-tokenizers.html).

```{r installations, warning=FALSE, eval=FALSE}
install.packages("cowplot")
install.packages("ggplot2")
install.packages("gridExtra")
install.packages("pdftools")
install.packages("plotly")
install.packages("quanteda")
install.packages("readr")
install.packages("SnowballC")
install.packages("tm")
install.packages("tokenizers")

```

#### coreNLP

"coreNLP" is a package I primarily use for part of speech tagging.  However, installing the package requires a large amount of time, so carefully consider whether you want to use these before running this script.  **The default here is to keep "eval=FALSE", so this code can only be run manually.**

```{r coreNLP installations, warning=FALSE, eval=FALSE}
install.packages("coreNLP") #only need to do this once
library(coreNLP)
downloadCoreNLP() #only need to do this once
initCoreNLP() #NOTE: this function will cause problems if you try to load it into the environment more than once.  Make sure to keep it out of loops too.

```

### Loading Libraries
The next step is to load the libraries for these packages into your environment, which is accomplished with the "library()" function.

```{r libraries, warning=FALSE, message=FALSE}
library(ggplot2)
library(quanteda)
library(pdftools)
library(plotly)
library(readr)
library(SnowballC)
library(tm)
library(tokenizers)
```